---
title: Estimating background QRP rate
author: ''
date: '2018-12-14'
slug: estimating-background-qrp-rate
categories:
  - analysis
tags:
  - QRP
---

## WORK IN PROGRESS -- DRAFT ##

Problem:

Have you ever done this... if so, how many times have you done *this*?   Frequency of engagement among researchers.

This tells us a few things:

1. Tells us about how common these practices are.
2. If it's a common practice, and/or, if people are doing this repeatedly, then it's likely that this is a more acceptabile practice (THAT person should have rated the practice as being defensible, and it's likely that the broader sample of researchers will also think that the practice is more acceptible).
3. What it *is* missing however, is that people author multiple papers. So 


People might have done this only a handful times, but only authored a handful of papers using this method (so the impact on the literature)


But how does this affect the literature? What is the proportion of the literature affected by a given QRP? We would like to know this because it tells us about how much faith, or confidence we can put in the literature. Tells us how severe the problem is.

Authors write multiple papers. We can't really get a sense of how prevalent a practice is within the literature given the way that I plan on asking questions about how often somebody engages in a QRP.

So the problem with usual statistical approaches to estimating prevalence or frequency, is that they...
What we want to do is treat each paper as... Want to estimate a proportion for each individual... rather than asking them the number of times and averaging over the number of people sampled. It doesn't account for the fact that people author multiple papers.

You *could* ask people about the number of papers they've authored, and the number of times in which they've done that practice. You could then arrive at some mean. But this doesn't account for uncertainty in our estimate of the prevalence rate due to our ability to detect with this survey -- whether due to false admissions, or due to other factors causing people to incorrectly answer:

- purposefully answering incorrectly
- can't remember number of papers authored
- can't recall or recalled incorrectly whether the practice was used

Problem: imperfect "detection" ... We cannot distinguish non-detection from a true negative, i.e. never having engaged in the practice. This will mean that our estimates will be biased.

So in order to account for our inability to adequately observe the true rate of engagement in QRP's within the survey sample, we can model the two processes explicitly and simultaneously using hierarchical modelling. Treating the 'state process' and the 'observation process' separately. The two processes are said to be hierarchical because parameters at one level govern parameters at the lower level.

Here is my hierarchical model:

## Modelling Description 

I follow the logic of a two-proces occupancy-detection model presented here: [https://cals.arizona.edu/~steidl/HM/2_Introduction_to_Hierarchical_Models_and_Occupancy.pdf](https://cals.arizona.edu/~steidl/HM/2_Introduction_to_Hierarchical_Models_and_Occupancy.pdf).

There are two types of response, ("Admitted" and "Did not Admit"):

```{r}
library(DiagrammeR)
node_df <-
  create_node_df(
    n = 7,
    type = "a",
    label = c("Admitted", "Did Not Admit", "Did QRP", "Did not do QRP", "Did it,\n did not admit", "Did not do it, \n but admitted \n (why tho)", "survey \n(period over \nwhich we are \neliciting \nresponses)"),
    style = "filled",
    color = c("aqua", "aqua", "pink", "pink", "pink", "pink", "purple"),
    shape = c("rectangle", "rectangle",
              "circle", "circle", "circle", "circle", "rectangle"),
    fontsize = 5,
    fillcolor = c("gray", "gray", "lightpink", "lightpink", "lightpink", "lightpink", "thistle"))

edge_df <- create_edge_df(from = c(1,1,2,2,7,7), to = c(6,3,4,5,1,2),style = c("dashed", "solid", "solid", "solid", "solid", "solid", "solid"))

graph <- create_graph(node_df, edge_df)
render_graph(graph, layout = "tree")
```

Uncertainty in admission can stem from the following:

```{r}
library(tidyverse)
library(pander)

tibble(individual = c(1:4), paper_1 = c(0,1,0,0), paper_2 = c(1,1,0,0), paper_3 = c(1,1,0,0), paper_4 = c(0,1,NA, 0)) %>% pander()
```


If the surveys were perfect, all 0's would indicate a true absence or true never having engaged in the QRP.

## Assumptions and potential problems:

One thing I am uncertain about is whether uncertainty in an individual's estimate is allowed to stem from the fact that we have uncertainty in the number of trials... i.e. number of papers they authored.

## Simulate some data

"The publication rate varies significantly among the individuals, and is not normally distributed" Rorstad et al. Can use their models to simulate publication rate data.

 ```{r}
# N <- 1000 # 1000 unique responses
# num_times
# individual_pub_rate
# simulated_data <- data_frame(responseID = 1:N, num_times)


```



