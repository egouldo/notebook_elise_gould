---
title: "PhD Research Proposal"
author: "Elise Gould"
date: '2017-02-15'
output:
  pdf_document: default
  html_document:
    keep_md: yes
categories: research proposal
slug: research_proposal
tags: project
bibliography: ../../static/files/citations/research_proposal.bib
number_sections: yes
---

Max 5 pages, 3000 - 4000 words.
Send to committee and cc'd to rhd-biosciences@unimelb.edu.au
- are topic and aims well defined and achievable
- are the appropriate methods established or will they need to be developed
- does the student understand the relevant literature
- are there any intellectual property issues
- current / potential EHS issues?
- well-referenced intro that explains scientific context of project (1 - 3 pages)
- an outline of the major aims of the project and the approaches that will be taken (1 - 2 pages)

Keywords:

- conservation
- decision-making
- ecology
- reproducibility
- structured decision-making

# Introduction

Successful biodiversity conservation and management is underpinned by effective and robust decision-making [@Mukherjee:2018cb]. Decision-makers are tasked with allocating limited resources in the face of uncertainty about the effectiveness of alternative management interventions, and incomplete or inadequate scientific information. Moreover, environmental decisions often must be made in complex socio-economic and political contexts, with multiple stakeholders and multiple and/or competing objectives. Formal, structured approaches to decision-making under uncertainty, such as Structured Decision Making, are commonly espoused as a means to more robust and reproducible conservation decisions because of their transparency in the decision process (**LIST EXAMPLES**). However, this claim to assumed reproducibility remains untested, and a more nuanced consideration of what exactly reproducibility means in this application is lacking. Given that science is said to be in the grips of a "reproducibility crisis", and that biased and unfair scrutiny in the broader socio-political climate of "alternative facts" threatens credibility of scientific knowledge [@Baillieul:2018jd], it is both essential and timely that the reproducibility of decision-support in ecology and conservation is given rigorous research attention.

Science is a cumulative process building upon previous research to develop knowledge, however, this process requires that the findings are both real, and replicable [@Nakagawa:2015bn]. As the recent "reproducibility crisis" debate illustrates, this assumption is often incorrect [@Nature:un]. Failure to reproduce a large proportion of published studies in the fields of psychology and medicine has received considerable attention and provoked heated discussion among researchers in the broader scientific community. For example, the Open Science Collaboration's Psychology Reproducibility Project reported that less than half of published results could be reproduced [@OpenScienceCollaboration:2015cn]. 

Despite a growing body of large-scale meta-analyses across many different disciplines, debate as to whether there is a "crisis" persists. Fanelli et al. [-@Fanelli:2018je] use a failed replication of a large-scale meta-analysis to argue that the "crisis" is mistaken, and should instead be re-branded as a narrative of "epochal changes and empowerment of science" [@Jamieson:2018kz]. In a 'post-truth' era of 'alternative-facts', how scientists communicate research on the robustness of science and its self-correcting mechanisms is certainly important [@Sutherland:2017hc]. Although large-scale direct replications are largely absent in ecology [@Nakagawa:2015bn], an initial assessment of the conditions found to foster reproducibility problems provide evidence that ecology as a discipline is *at risk* of a "reproducibility crisis" [@Fidler:2017he]. **Moreover, CITE HANNAH'S PAPER HERE.** **CALL FOR METARESEARCH, fidler**.

## Research scope: reproducibility *of* decisions versus reproducibility *for* decision-making

Morrison et al. [-@Morrison:2016cd] outline the potential implications of the non-reproducibility of the technical inputs into the decision-making process. Firstly, Unreliable predictions may result in finite resources being directed inefficiently, and therefore in opportunity costs. Secondly, non-reproducible predictions undermine the credibility of the original model, and therefore any decisions resulting informed by that evidence. Thirdly, the effectiveness of an intervention may not be able to be evaluated against the predictions of the model if they are not reproducible. This is especially pertinent in ecology and conservation, where true randomised experimental design is often infeasible, and the performance of conservation actions must be measured against the counter-factual -- often estimated using predictive models [@Law:2017ia]. Finally, non-reproducible predictions prevent decision-makers from comparing revised models with updated parameters against previous models. This is relevant for contexts where decision-making is informed by ongoing monitoring programmes, such as in adaptive management-based conservation. These arguments echo earlier work in evidence-based conservation decision-making (CDM) literature underscoring the importance of a robust evidence-base for informing management (**cite examples: Law, gardner, Cullina, sutherland S4.**).

Reproducibility research in ecology and conservation should certainly continue advancing for the primary evidence-base. However, the scope of research should extend beyond evidence-generation, to examine reproducibility issues around the use of that evidence to inform decision-making. Evidence is collated, synthesised, and transformed into decisions, ideally using some formal decision support system [-@Gardner:2018dm]. It is this broader decision process in which conservation decisions are made that needs to be examined in terms of reproducibility, not just the technical elements of the decision process. Decision-making is a "human enterprise" shaped by values and expectations [@Mukherjee:2018cb] and failing to integrate the human elements of decision-making in conservation may lead to sub-optimal outcomes [@Bennett:2017jh]. For example, scenario analyses are often used to make a case for implementing a particular management strategy. However, Law et al. [-@Law:2017ia] emphasise that the selection of appropriate scenarios may influence inferences about the appropriate course of action -- analysts must ensure that scenarios are not impossible or highly improbable, because this can give the impression that a particularly positive or negative outcome is likely. Problem formulation is a critical phase of the decision process but is often poorly documented or even completely neglected when it comes to published applications of decision support systems in ecology and conservation. Human values shape preferences about the acceptability of decision outcomes in terms of fulfilling conservation objectives. Decision modelling outputs may be sensitive to the specification of the performance measure (e.g. Giljohann et al. [-@Giljohann:2014tv]). It is vital that performance measures properly incorporate decision-maker and stakeholder values, and that this is given proper attention and documentation during problem formulation. These examples above demonstrate that during the development of a decision-support system, there are various 'decision-points' faced by decision analysts that may influence both the decision-outcome and reproducibility of that DSS.

## Research aims and objectives

This aim of this PhD is to investigate the transparency and reproducibility of decisions generated from structured approaches to conservation decision-making (such as SDM). By taking as its unit of observation / focus decision-support systems (DSS's), this research will identify critical points in the decision process that threaten the reproducibility of ecological decisions and undermine conservation success. The first major objective of this research is to expand and develop a conceptual understanding of "reproducibility" that is fit for application in conservation decision-making and translational research in ecology. Existing approaches to reproducibility exclusively focus on hypothesis-testing based research, however in applied ecology and conservation technical approaches draw primarily on decision-analytic tools and methods for decision-making. Secondly, I will systematically review the published literature on decision support systems to evaluate the magnitude and extent of the "reproducibility crisis" in translational research of ecology and conservation. Presently, the third major aim of this research is largely unclear, but this work would like to design and implement (potentially in a pilot) some form of intervention aimed at trying to improve the reproducibility of decision support tools in ecology and conservation. In the remainder of this proposal I identify knowledge gaps in the literature and highlight tangible work that I can do in my PhD to fill these gaps.

Provide further insight and deepen the understanding of issues of reproducibility and transparency in ecology and conservation decision making.

1. Develop an understanding / framework / definition of reproducibility that is applicable to Non-NHST and translational research, that is sensitive to the particularities of the data, the methods, the questions commonly used within these contexts.
2. Build a picture / conceptual model of ecological informatics with a reproducibility bent, multi-level.
2. Characterise the extent and magnitude of the "problem."reproducibility crisis"
3. Devise (and possibly test) interventions for improving reproducibility and transparency in ecology and conservation. improving the scientific process in ECO / CDM. Could be policy-based, could be guidelines  targeted at the author, could be a technical tool.

# Reproducibility for decision support in ecology and conservation: towards a formal typology 

The term "reproducibility" broadly encompasses the ability to "replicate an experiment or study and/or its outcomes" [@Fidler:2017he]. A number of different typologies of reproducibility have been proposed, with each 'type' of reproducibility having its own role or function in terms of the type of knowledge it generates. For example, Nakagawa and Parker [-@Nakagawa:2015bn] distinguish between "exact" or "close", "partial", "conceptual" and "quasi" replications, in order of decreasing fidelity to the original work. Conceptual replications are tests that follow the same hypotheses, whereas quasi-replications replicate across species or systems to test for the generality of biological phenomena across species or systems. According to this typology, the two main functions of the different replications are to test for validity or generality, with there being a distinct trade-off between them.

Others have defined "reproducible research" or a set of minimum standards requiring that data and code is made available for others to verify results of the original study and for re-use of the data [@Peng:2009vn]. This type of "computational reproducibility" has been defined as "re-performing the same analysis with the same code using a different analyst" [@Patil:2016tm]. This has been advocated for as the minimum standard when there are a lack of time or resources for intensive replications [@Williams:2017bg]. Reproducible research emphasising computational reproducibility seems to comprise the bulk of the reproducibility literature in ecology, perhaps because replications in ecology have typically been considered infeasible due to the inherent spatio-temporal variation in nature, or even unethical for work on threatened species [@Nakagawa:2015bn].

## Unpacking "reproducibility" for decisions

Transparency is frequently touted as both a key-feature of environmental decision-support systems, and as central to delivering reproducible decisions [@Kim:2016gf] (OTHER REFS, basically anything SDM). However, the discussion of reproducibility in these contexts usually stops here, and there is no further discussion as to some definition of the term. [-@McIntosh:2011ew] elaborates further than most to explain the link between transparency and reproducibility in formal decision support for ecology: 

> Transparent because rational explanations can be provided to support decisions, and because the user/stakeholder/citizen can reproduce the decision procedure, play with the weights, and perform sensitivity analysis to assess decision strength and robustness.

Examining this statement reveals that, at least for these authors, reproducibility is simply the repeatability of the decision procedure. The repeatability of a decision procedure does not necessarily ensure that the resultant "decision" from the procedure can be repeated (computational reproducibility). I expect that the more complicated the decision process (and the more complex the decision problem / domain), the less likely the decision may be reproduced. Repeatability *is* important, and the assumption that the transparency of decision processes implemented informal decision support systems results in them being repeatable should be tested -- especially given how commonly it is used as a selling-point for particular approaches, like SDM. However, I believe that a more thorough treatment should be given to defining reproducibility for decision support in ecology and conservation.

## Moving beyond "computational reproducibility"

Much reproducibility research in ecology has been targeted at individual researchers [@Fraser:2018cl] and considers reproducibility to be computational in nature. This research focuses on the data analysis pipeline for a single analysis or study, taking the starting point for reproducibility to be where analysis is ready to begin (data is on hand and probably digitised). Drawing on work from computational biology and scientific computing, it offers technological solutions and is largely software and data focussed. These approaches seem to resonate among ecologists, with much reflection and discussion in the grey literature, particularly in blog and twitter format. For example, Wilson et al. [-@Wilson:2014ck] describe a suite of practices for improving the reproducibility of analyses. Suggested practices include version control, incremental changes, functional programming and unit-testing. Others have emphasised the use of literature programming techniques and containerised analyses that link code to their inferences in a single document [@Baumer:2015hc; @Gandrud:2016ux]. Noble et al. [-@Noble:2009da] describe how to organise computational biology projects while the British Ecological Society's guide to data management synthesises various tools and solutions for making research reproducible into a single guide [@BritishEcologicalSociety:2014va]. 

The "open data" movement within ecology, and science more broadly, assumes more of a systems-level view of reproducibility: situating the reproducibility of individual studies within the broader life-cycle of scientific research. Some computational ecology research in this context is still targeted at improving the research practices of individuals, however, the overarching aim is to improve the computational reproducibility or research in order to facilitate large-scale meta-analyses [@Merali:2010tw], longitudinal studies and data synthesis [@White:2013ea]. Solutions arising from the open data movement are a mixture of technological and policy-based. For instance, White et al. [-@White:2013ea] discuss the particularities of ecological data and describe methods for ensuring data is re-usable to others; while Ram [-@Anonymous:a1kBf1q9] illustrates how git and GitHub can facilitate greater reproducibility and increased transparency. Similarly, Whitlock [-@Whitlock:cl] describes the technical and cultural issues that must be addressed to increase data-archiving practices in ecology and evolution, including journal policy. The task of reproducibility is thus multi-pronged and responsibility is shared among individual researchers, as both data creators and users (e.g. researchers synthesising data), journal editors and reviewers for improving reproducibility via openness practices and policy [@MoruetaHolme:2018bi]. 

By devising a typology of reproducibility applicable to decision support systems, this work will advance reproducibility and transparency research in ecology beyond computational reproducibility.

------

**Chapter One: Defining a typology of reproducibility for decision support systems in conservation and ecology**

We cannot go about the task of evaluating the reproducibility of decision support systems without some definition and measure of what that might mean in this context. A major aim of this project is to propose a typology of reproducibility for decision support systems, considering what their role and function might be in terms of the knowledge they generate. At present, what format this work will take is unclear -- this work could be embedded in other chapters (systematic review or the replication case study), or be a standalone chapter.

Given that the decision-support tools used during decision-making are not usually hypothesis-based, and their outputs are not usually *p* values, the first step will likely involve determining the unit of measure for reproducibility. Although the 'decision outcome' (the decision recommended at the end of the decision process) seems to be a good candidate, this will need careful consideration for conceptual replications -- should a conceptual replication generate differences during the problem formulation phase, it is possible that the set of potential decision alternatives might not even match those in the original study. 

Conceptual replications are typically constrained by the same hypothesis [@Nakagawa:2015bn], but what would the equivalent be for a DSS It could be that an equivalent for DSSs is the problem formulation phase. However, because the decision outcome is often sensitive to the specification of the problem, it is probably important that there remains scope for variation in this phase -- analysts vary in their ability to properly elicit the problem specification from decision-makers. For instance, the fundamental objective(s) might not be properly specified, the analyst might be subject to 'evidence complacency' [see @Sutherland:2017hc]  or be anchored by the alternatives proposed by the decision-maker, ignoring existing evidence and knowledge about the full range of potential decision alternatives for a given problem. 

Thus, for each relevant type of reproducibility for DSSs, the set of constraints and conditions that constitute the replication need to be carefully specified, in reference to the overall function of that type of replication. The direct replication is the most obvious in terms of its function - it would lend validity to the decision(s) recommended by the original DSS and credibility to the decision process that led to that decision. However, with increasing shift away from fidelity to the original study, the purpose of the replication remains unclear. Does a conceptual replication that converges on the same decision as the original DSS give validity to the original decision, and / or does it tell us something about the applicability of the particular suite of tools built into the DSS for a given decision-context? 

------

# Identifying reproducibility issues for decision support in ecology and conservation: non-hypothesis testing based research

The reproducibility literature has focused exclusively on hypothesis-testing, whether that be Bayesian or frequentist. This also applies to initial research focusing on ecology and evolution. However, Fidler [-@Fidler:2016wv] correctly identifies that in applied ecological research, particularly in conservation science, non-hypothesis testing methods, such as decision-theory, cost-effectiveness analysis, optimization and other scientific computing methods are common. These approaches come with their own set of reproducibility issues. However, a full understanding of the types of reproducibility issues, as well as their impact on either the evidence-base, or the decisions informed by the evidence-base, is yet to emerge. 

In what is likely the first reproducibility study in conservation, Morrison et al. [-@Morrison:2016cd] evaluate the Population Viability Analysis (PVA) literature by performing direct tests of repeatability and reproducibility. They found that poor model parameter reporting practices meant a large number could not even be repeated, let alone reproduced. This work marks an important step towards identifying reproducibility issues relevant not only to PVA-based research, but also to CDM more broadly. Outside of ecology, where reproducibility research has received much more attention and focus, I could only identify a single paper that considered non-NHST based work in the reproducibility discussion. Crutzen and Peters [-@Crutzen:2017kn] suggest re-terming "power" and "underpowered" studies as being "undersamplesized", in a move to embrace the disciplinary shift away from NHST towards inferences based on confidence intervals for effect size estimates. They exemplify such research as studies aiming to obtain accurate power estimates. Although their treatment of the issue is rather cursory, it is promising to see the scope of reproducibility research expanding to non-NHST approaches in broader science. However, it is pertinent that reproducibility research in ecology and CDM: we cannot evaluate the reproducibility of this body of work without first identifying relevant types of reproducibility issues.

------

**Chapter Two: QRPs for non-hypothesis testing research**

The first output for this research is to generate a "roadmap" of reproducibility issues, biases and questionable research practices (QRPs) that are encountered when developing Decision Support Systems in ecology and conservation.In this chapter I will attempt to identify where exactly in the DSS development process particular biases or 'decision-points' are likely to occur. This work should serve as a launching point for proposing standards and technical solutions targeted at individual (or teams of) researchers / analysts, with the broader objective of minimising the extent and magnitude of questionable research practices for DSS. 

This chapter will constitute (from what I am aware of) the first attempt to investigate reproducibility issues for non-hypothesis testing based research. To this extent, the knowledge resulting from this chapter should also be applicable to other fields of science beyond ecology and conservation where translational research and non-hypothesis testing methods are prevalent. It will also build on the work of Fraser et al. [-@Fraser:2018cl] in advancing research on reproducibility and transparency in ecology.

*Methods for generating the roadmap*

1. Sketch out the SDM / DSS development process, but breakdown into modelling steps if necessary. As a starting point, I will take the Structured Decision Making process as the overarching framework for building a Decision Support System.
        a. Does this process differ for different tools / decision frameworks?
2. Identify sources of bias AND QRPs that others have identified in ecology and evolution, but also in other scientific disciplines and translational research fields
        a. at the individual DSS level
        b. at a higher level, e.g. in the evidence-base
        d. Are these biases / QRPs applicable to DSS's?
        e. are there other biases unique to DSS's that haven't been considered?
        f. Is their occurrence specific to the type of tool or application under consideration? i.e. are some tools more robust to biases / QRPs than others?
3. Map these biases / QRPs onto 1, where do they occur at the various decision-points?

I plan on utilising the outputs of the reproducibility discussion session planned for the Qaecera Retreat. The aim of the session is to initiate awareness and discussion among Qaecologists and Cebranalysts about reproducibility in our research practices, with the subsequent aim of equipping people with solutions to minimise or overcome these issues. The format for the session will involve structured / facilitated discussion in small break-out groups. Given the collaborative nature of this work, participants will be invited to co-author the paper after the retreat. This might also incentivise people to attend the session!

-------

# Evaluating the Transparency and likely reproducibility of decisions in ecology and conservation

Fidler et al. [-@Fidler:2017he] call for an assessment of the completeness and transparency of methodological and statistical reporting in journals for ecology. Such assessments should take the form of extensive journal surveys, with the aim of highlighting deficiencies in journal's reporting policies. Why is it important: Incomplete reporting impedes direct replication, meta-analysis, and also direct re-analysis projects. From personal experience, I have reason to believe that incomplete reporting is rife in environmental decision support systems. In working with Bayesian Belief Networks for catchment management, it was impossible to re-build all but one system model from the published literature. At best, the causal structure of the model is reproducible, but only one published study I encountered contained sufficient information so as to reproduce its parameterisation. Importantly, there is also the issue of understanding modeller choices about how and why variables were parameterised. This information is rarely recorded, and often the source of empirical data used in parameterisation is not reported. The next chapter of my research will take up this call, where I will conduct a systematic review aiming to systematically evaluate the decision support literature in ecology and conservation for its transparency and likely reproducibility.

------

**Chapter Three: Systematic Review**

The roadmap of QRPs developed in Chapter Two will inform both the scoping and evaluation criteria for the systematic review. The results of the review will be used to generate a 'reproducibility check-list' or set of criteria for DSS in ecology and conservation. The check-list will be aimed at individual researchers, but with the hope of being adopted by relevant journals. The check-list will need to balance a sufficiently acceptable set of standards against resource constraints on individual researchers in order to prevent the check-list from deterring important DSS work from being published.

*Methods:*

I will follow guidelines for undertaking a systematic review in the format and procedure established by Collaboration for Environmental Evidence (CEE) Evidence Synthesis [@CEE:M-Sg3G8U]. I chose this method of systematic review partially because the topic area is relevant (environmental management), and partially because of the comprehensiveness of the guidelines: the guidelines provides detailed and systematic methods for developing the search strategy, inclusion / exclusion criteria, and coding criteria for evaluating the literature, including pilot searches for refining criteria. The other draw card was the enforced development and pre-registration of the protocol for conducting the review in order to prevent 'mission creep' as the review progresses.

In addition, I will use the PRISMA statement for reporting (CITE), as recommended by Nakagawa and Poulin [-@Nakagawa:2012fl]. The PRISMA check-list contains a check-list of 27 reporting items, as well as a flow diagram that visualizes the database searching procedure as well as decisions for including and evaluating studies. The aim of the statement is to increase the transparency of the literature review process.

*Review scope:*

I distinguish between 'decision *frameworks*' and 'decision *tools*': frameworks may be considered the structured set of procedures governing the approach to an entire decision process - i.e. from problem formulation, to modelling the consequences of decision alternatives, to evaluating alternatives. Structured Decision Making (SDM) is an example of a decision framework [@Bower:2017im]. Decision tools may be understood as the particular procedures or methods used to solve or provide inputs to inform a particular phase in the overarching decision framework. For example, Bayesian Networks, Population Viability Analyses and Multiple Multiple Criteria Decision Analysis constitute decision tools. This review will investigate the reproducibility of decisions generated from 'decision *frameworks*', rather than just the reproducibility of 'decision *tools*'.


*Analysis and Reporting:*

By focusing on decision frameworks more broadly -- rather than a particular framework, like SDM -- I hope to be able to evaluate whether different frameworks might influence the transparency and likely reproducibility of resultant decisions. This should also provide a means of testing the claim that SDMs are increase transparency, and therefore the reproducibility of decisions. Importantly, in not restricting the scope too heavily, this provides a way for dealing with linguistic ambiguity around the language of decision support in ecology and conservation. For example, there is considerable slippage between the terms 'decision support system' and 'decision support tools.' Although SDM approaches are very common in conservation, restricting the inclusion of studies to nominally SDM studies will likely omit much research that might be decision-analytic in approach, and therefore still constitute a comparable structured or formal framework for decision-making. Following a pilot test of the searching and inclusion criteria, the scope might be revised (i.e. if too many papers are returned).

I am presently unclear on what analyses to conduct. But they will probably include the presentation of some descriptive statistics based around critical evaluation criteria.

------

**Chapter Four: case studies in replicating DSS's**

The final chapter will involve the use of case studies of real-world ecological and conservation decisions. 

a) demonstrate an application of the typology of reproducibility developed in Chapter 1
b) proof-of-concept for replicating a decision support tool.


One thing to test for in the replication:



Fidler: re-analysis projects - both direct and conceptual re-analyses ("highlight the impact of disclosed and undisclosed statistical assumptions and versions or editions of software, [etc..]")

Conceptual re-analysis: "provide the opportunity to assess the extent of variability among analytic approaches and choices".

Comparative - compare a selection of different DSS problems.

**Potential Case Studies**

Presently there are several different opportunities for case studies.

1. Chris Jones - 

1. CJ - multiple river systems, with multiple management agencies opportunity for a red card type of experiment?

2. NESP -- Use my involvement in this project to examine QRPs sources of biases in a specific case study? What's the test?


------

# Timeline of activities and goals

- timeline of activities and goals for first 12 months of candidature (GANTT chart),
including preparation of the confirmation report and holding the confirmation meeting (9 - 10 months).
- TBD

# References
