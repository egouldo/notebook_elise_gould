---
title: "PhD Research Proposal"
author: "Elise Gould"
date: '2017-02-15'
output:
  pdf_document: default
  html_document:
    keep_md: yes
categories: research proposal
slug: research_proposal
tags: project
bibliography: ../../static/files/citations/research_proposal.bib
number_sections: yes
---

Max 5 pages, 3000 - 4000 words.
Send to committee and cc'd to rhd-biosciences@unimelb.edu.au

- are topic and aims well defined and achievable
- are the appropriate methods established or will they need to be developed
- does the student understand the relevant literature
- are there any intellectual property issues
- current / potential EHS issues?
- well-referenced intro that explains scientific context of project (1 - 3 pages)

- an outline of the major aims of the project and the approaches that will be taken (1 - 2 pages)

Keywords:

- conservation
- decision-making
- ecology
- reproducibility
- structured decision-making

# Introduction

Successful biodiversity conservation and management is underpinned by effective and robust decision-making [@Mukherjee:2018cb]. Decision-makers are tasked with allocating limited resources in the face of uncertainty about the effectiveness of alternative management interventions, and incomplete or inadequate scientific information. Moreover, environmental decisions often must be made in complex socio-economic and political contexts, with multiple stakeholders and multiple and/or competing objectives. Formal, structured approaches to decision-making under uncertainty, such as Structured Decision Making, are commonly espoused as a means to more robust and reproducibile conservation decisions because of their transparency in the decision process (**LIST EXAMPLES**). However, this claim to assumed reproducibility remains untested, and a more nuanced consideration of what exactly reproducibility means in this application is lacking. Given that science is said to be in the grips of a "reproducibility crisis", and that biased and unfair scrutiny in the broader socio-political climate of "alternative facts" threatens credibility of scientific knowledge [@Baillieul:2018jd], it is both essential and timely that the reproducibility of decision-support in ecology and conservation is given rigorous research attention.

Science is a cumulative process building upon previous research to develop knowledge, however, this process requires that the findings are both real, and replicable [@Nakagawa:2015bn]. As the recent "reproducibility crisis" debate illustrates, this assumption is often incorrect [@Nature:un]. Failure to reproduce a large proportion of published studies in the fields of psychology and medicine has received considerable attention and provoked heated discussion among researchers in the broader scientific community. For example, the Open Science Collaboration's Psychology Reproducibility Project reported that less than half of published results could be reproduced [@OpenScienceCollaboration:2015cn]. 

Despite a growing body of large-scale meta-analyses across many different disciplines, debate as to whether there is a "crisis" persists. Fanelli et al. [-@Fanelli:2018je] use a failed replication of a large-scale meta-analysis to argue that the "crisis" is mistaken, and should instead be rebranded as a narratvive of "epochal changes and empowerment of scients" [@Jamieson:2018kz]. In a 'post-truth' era of 'alternative-facts', how scientists communicate research on the robustness of science and its self-correcting mechanisms is certainly important [@Sutherland:2017hc]. Although large-scale direct replications are largely absent in ecology [@Nakagawa:2015bn], an initial assessment of the conditions found to foster reproducibility problems provide evidence that ecology as a discipline is *at risk* of a "reproducibility crisis" [@Fidler:2017he]. **Moreover, CITE HANNAH'S PAPER HERE.** **CALL FOR METARESEARCH, fidler**.

## Research scope: reproducibility *of* decisions versus reproducibility *for* decision-making

Morrison et al. [-@Morrison:2016cd] outline the potential implications of the non-reproducibility of the technical inputs into the decision-making process. Firstly, Unreliable predictions may result in finite resources being directed inefficiently, and therefore in opportunity costs. Secondly, non-reproducible predictions undermine the credibility of the original model, and therefore any decisions resulting informed by that evidence. Thirdly, the effectiveness of an intervention may not be able to be evaluated against the predictions of the model if they are not reproducible. This is especially pertinent in ecology and conservation, where true randomised experimental design is often unfeasible, and the performance of conservation actions must be measured against the counterfactual -- often estimated using predictive models [@Law:2017ia]. Finally, non-reproducible predictions prevent decision-makers from comparing revised models with updated parameters against previous models. This is relevent for contexts where decision-making is informed by ongoing monitoring programmes, such as in adaptive management-based conservation. These arguments echo earlier work in evidence-based conservation decision-making (CDM) literature underscoring the importance of a robust evidence-base for informing management (**cite examples: Law, gardner, Cullina, sutherland S4.**).

Reproducibility research in ecology and conservation should certainly continue advancing for the primary evidence-base. However, the scope of research should extend beyond evidence-generation, to examine reproducibility issues around the use of that evidence to inform decision-making. Evidence is collated, synthesised, and transformed into decisions, ideally using some formal decision support system [-@Gardner:2018dm]. It is this broader decision process in which conservation decisions are made that needs to be examined in terms of reproducibility, not just the technical elements of the decision process. Decision-making is a "human enterprise" shaped by values and expectations [@Mukherjee:2018cb] and failing to integrate the human elements of decision-making in conservation may lead to sub-optimal outcomes [@Bennett:2017jh]. For example, scenario analyses are often used to make a case for implementing a particular management strategy. However, Law et al. [-@Law:2017ia] emphasise that the selection of appropriate scenarios may influence inferences about the appropriate course of action -- analysts must ensure that scenarios are not impossible or highly improbablye, because this can give the impression that a particularly positive or negative outcome is likely. Problem formulation is a critical phase of the decision process but is often poorly documented or even completely neglected when it comes to published applications of decision support systems in ecology and conservation. Human values shape preferences about the acceptability of decision outcomes in terms of fulfilling conservation objectives. Decision modelling outputs may be sensitive to the specification of the performance measure (e.g. Giljohann et al. [-@Giljohann:2014tv]). It is vital that performance measures properly incorporate decision-maker and stakeholder values, and that this is given proper attention and documentation during problem formulation. These examples above demonstrate that during the development of a decision-support system, there are various 'decision-points' faced by decision analysts that may influence both the decision-outcome and reproducibility of that DSS.

## Research aims and objectives

This aim of this PhD is to investigate the transparency and reproducibility of decisions generated from structured approaches to conservation decision-making (such as SDM). By taking as its unit of observation / focus decision-support systems (DSS's), this research will identify critical points in the decision process that threaten the reproducibility of ecological decisions and udermine conservation success. The first major objective of this research is to expand and develop a conceptual understanding of "reproducibility" that is fit for application in conservation decision-making and translational research in ecology. Existing approaches to reproducibility exclusively focus on hypothesis-testing based research, however in applied ecology and conservation technical approaches draw primarily on decision-analytic tools and methods for decision-making. Secondly, I will systematically review the published literature on decision support systems to evaluate the magnitude and extent of the "reproducibility crisis" in translational research of ecology and conservation. Presently, the third major aim of this research is largely unclear, but this work would like to design and implement (potentially in a pilot) some form of intervention aimed at trying to improve the reproducibility of decision support tools in ecology and conservation. In the remainder of this proposal I identify knowledge gaps in the literature and highlight tangible work that I can do in my PhD to fill these gaps.

Provide further insight and deepen the understanding of issues of reproducibility and transparency in ecology and conservation decision making.

1. Develop an understanding / framework / definition of reproducibility that is applicable to Non-NHST and translational research, that is sensitive to the particularities of the data, the methods, the questions commonly used within these contexts.
2. Build a picture / conceptual model of ecological informatics with a reproducibility bent, multi-level.
2. Characterise the extent and magnitude of the "problem."reproducibility crisis"
3. Devise (and possibly test) interventions for improving reproducibility and transparency in ecology and conservation. improving the scientific process in ECO / CDM. Could be policy-based, could be guidelines  targeted at the author, could be a technical tool.

# Reproducibility for decision support in ecology and conservation: towards a formal typology 

The term "reproducibility" broadly encompasses the ability to "replicate an experiment or study and/or its outcomes" [@Fidler:2017he]. A number of different typologies of reproducibility have been proposed, with each 'type' of reproducibility having its own role or function in terms of the type of knowledge it generates. For example, Nakagawa and Parker [-@Nakagawa:2015bn] distinguish between "exact" or "close", "partial", "conceptual" and "quasi" replications, in order of decreasing fidelity to the original work. Conceptual replications are tests that follow the same hypotheses, whereas quasi-replications replicate across species or systems to test for the generality of biological phenomena across species or systems. According to this typology, the two main functions of the different replications are to test for validity or generality, with there being a distinct trade-off between them.

Others have defined "reproducible research" or a set of minimum standards requiring that data and code is made available for others to verify results of the original study and for re-use of the data [@Peng:2009vn]. This type of "computational reproducibility" has been defined as "re-performing the same analysis with the same code using a different analyst" [@Patil:2016tm]. This has been advocated for as the minimum standard when there are a lack of time or resources for intensive replications [@Williams:2017bg]. Reproducible research emphasising computational reproducibility seems to comprise the bulk of the reproducibility literature in ecology, perhaps because replications in ecology have typically been considered infeasible due to the inherent spatio-temporal variation in nature, or even unethical for work on threatened species [@Nakagawa:2015bn].

## Unpacking "reproducibility" for decisions

Transparency is frequently touted as both a key-feature of environmental decision-support systems, and as central to delivering reproducible decisions [@Kim:2016gf] (OTHER REFS, basically anything SDM). However, the discussion of reproducibility in these contexts usually stops here, and there is no further discussion as to some definition of the term. [-@McIntosh:2011ew] elaborates further than most to explain the link between transparency and reproducibility in formal decision support for ecology: 

> Transparent because rational explanations can be provided to support decisions, and because the user/stakeholder/citizen can reproduce the decision procedure, play with the weights, and perform sensitivity analysis to assess decision strength and robustness.

Examining this statement reveals that, at least for these authors, reproducibility is simply the repeatability of the decision procedure. The repeatability of a decision procedure does not necessarily ensure that the resultant "decision" from the procedure can be repeated (computational reproducibility). I expect that the more complicated the decision process (and the more complex the decision problem / domain), the less likely the decision may be reproduced. Repeatability *is* important, and the assumption that the transparency of decision processes implemented informal decision support systems results in them being repeatable should be tested -- especially given how commonly it is used as a selling-point for particular approaches, like SDM. However, I believe that a more thorough treatment should be given to defining reproducibility for decision support in ecology and conservation.

## Moving beyond "computational reproducibility" in ecology

Most reproducibility research in ecology has been targeted at individual researchers [@Fraser:2018cl] and considers reproducibility to be computational in nature. This research focuses on the data analysis pipeline for a single analysis or study, taking the starting point for reproducibility to be where analysis is ready to begin (data is on hand and probably digitised). Drawing on work from computational biology and scientific computing, it offers technological solutions and is largely software and data focussed. These approaches seem to resonate among ecologists, with much reflection and discussion in the grey literature, particularly in blog and twitter format. For example, Wilson et al. [-@Wilson:2014ck] describe a suite of practices for improving the reproducibility of analyses. Suggested practices include version control, incremental changes, functional programming and unit-testing. Others have emphasised the use of literature programming techniques and containerised analyses that link code to their inferences in a single document [@Baumer:2015hc; @Gandrud:2016ux]. Noble et al. [-@Noble:2009da] describe how to organise computational biology projects while the British Ecological Society's guide to data management synthesises various tools and solutions for making research reproducible into a single guide [@BritishEcologicalSociety:2014va]. 

The "open data" movement within ecology, and science more broadly, assumes more of a systems-level view of reproducibility: situating the reproducibility of individual studies within the broader life-cycle of scientific research. Some computational ecology research in this context is still targeted at improving the research practices of individuals, however, the overarching aim is to improve the computational reproducibility or research in order to facilitate large-scale meta-anlyses [@Merali:2010tw], longitudinal studies and data synthesis [@White:2013ea]. Solutions arising from the open data movement are a mixture of technological and policy-based. For instance, White et al. [-@White:2013ea] discuss the particularities of ecological data and describe methods for ensuring data is re-usable to others; while Ram [-@Anonymous:a1kBf1q9] illustrates how git and GitHub can facilitate greater reproducibility and increased transparency. Similarly, Whitlock [-@Whitlock:cl] describes the technical and cultural issues that must be addressed to increase data-archiving practices in ecology and evolution, including journal policy. The task of reproducibility is thus multi-pronged and responsibility is shared among individual researchers, as both data creators and users (e.g. researchers synthesising data), journal editors and reviewers for improving reproducibility via openness practices and policy [@MoruetaHolme:2018bi]. 

However, even policy-based approaches to transparency and reproducibility still only consider methodological or computational reproducibility issues in ecology. 

- broader research context - openness standards - see parker and nakagawa TOP guidelines.

Some work has examined the barriers to individual researchers publishing their code, i.e. "source-code witholding". These barriers include (institutional support, funding policy, publishing requirement - journal level) [@Science:2012hf].
Scientific computing studies also consider this broader ecosystem (R2 paper), and address issues of policy: [@Stodden:2012ww]. Highlights jouranl poolicy and funding agency, but misses publishing requirements, as the paper above included. This study also argues that it's not just about source-code but about the whole process, from the dev environment to the complete set of instructions that generated the figures, and paper.
- post-publication data sharing: publication level standards / policy, licensing schema and accompanying data and resource-sharing infrastructure, incentives [@Gribbin:2009us]
- Empirical evidence looking at trends in openness policy in terms of data sharing, in order to encourage: [@Stodden:2013ho].

WHY is this the case? seems infeasible.

What are alternative types?

QRP study: [@Agnoli:2017kl]

"The combination of formal systems for tracking provenance, and fed- erated data repositories like DataONE that provide unique identifiers for every data object, will be instrumental in realizing the goal of fully reproduc- ible science in support of understanding global environmental issues such as climate change, species invasions, and epidemics" data sharing should facilitate "as well as greater reproducibility and transparency of the methods and analyses that support scientific insights."
Cullina.


Both metaresearch work and reproducible research / open science / data advocates tend to consider reserach in the broader context, and offer policy-based approaches that target the journal editors, funding agencies, and publishing requirements (some technical, e.g. post-publication paper).

------

**Chapter One: Defining a typology of reproducibility for decision support systems in conservation and ecology**

We cannot go about the task of evaluating the reproducibility of decision support systems without some definition and measure of what that might mean in this context. A major aim of this project is to propose a typology of reproducibility for decision support systems, considering what their role and function might be in terms of the knowledge they generate. At present, what format this work will take is unclear -- this work could be embedded in other chapters (systematic review or the replication case study-=), or be a standalone chapter.

Given that the decision-support tools used during decision-making are not usually hypothesis-based, and their outputs are not usually *p* values, the first step will likely involve determining the unit of measure for reproducibility. Although the 'decision outcome' (the decision recommended at the end of the decision process) seems to be a good candidate, this will need careful consideration for conceptual replications -- should a conceptual replication generate differences during the problem formulation phase, it is possible that the set of potential decision alternatives might not even match those in the original study. 

Conceptual replications are typically constrained by the same hypothesis [@Nakagawa:2015bn], but what would the equivalent be for a DSS It could be that an equivalent for DSSs is the problem formulation phase. However, because the decision outcome is often sensitive to the specification of the problem, it is probably important that there remains scope for variation in this phase -- analysts vary in their ability to properly elicit the problem specification from decision-makers. For instance, the fundamental objective(s) might not be properly specified, the analyst might be subject to 'evidence complacency' [see @Sutherland:2017hc]  or be anchored by the alternatives proposed by the decision-maker, ignoring existing evidence and knowledge about the full range of potential decision alternatives for a given problem. 

Thus, for each relevant type of reproducibility for DSSs, the set of constraints and conditions that constitute the replication need to be carefully specified, in reference to the overall function of that type of replication. The direct replication is the most obvious in terms of its function - it would lend validity to the decision(s) recommended by the original DSS and credibility to the decision process that led to that decision. However, with increasing shift away from fidelity to the original study, the purpose of the replication remains unclear. Does a conceptual replication that converges on the same decision as the original DSS give validity to the original decision, and / or does it tell us something about the applicability of the particular suite of tools built into the DSS for a given decision-context? 

------

# Identifying reproducibility issues for decision support in ecology and conservation: non-hypothesis testing based research

The reproducibility literature has focused exclusively on hypothesis-testing, whether that be Bayesian or frequentist. This also applies to initial research focusing on ecology and evolution. However, Fidler [-@Fidler:2016wv] correctly identifies that in applied ecological research, particularly in conservation science, non-hypothesis testing methods, such as decision-theory, cost-effectiveness analysis, optimization and other scientific computing methods are common. These approaches come with their own set of reproducibility issues. However, a full understanding of the types of reproducibility issues, as well as their impact on either the evidence-base, or the decisions informed by the evidence-base, is yet to emerge. 

In what is likely the first reproducibility study in conservation, Morrison et al. [-@Morrison:2016cd] evaluate the Population Viability Analysis (PVA) literature by performing direct tests of repeatability and reproducibility. They found that poor model parameter reporting practices meant a large number could not even be repeated, let alone reproduced. This work marks an important step towards identifying reproducibility issues relevant not only to PVA-based research, but also to CDM more broadly. Outside of ecology, where reproducibility research has received much more attention and focus, I could only identify a single paper that considered non-NHST based work in the reproducibility discussion. Crutzen and Peters [-@Crutzen:2017kn] suggest re-terming "power" and "underpowered" studies as being "undersamplesized", in a move to embrace the disciplinary shift away from NHST towards inferences based on confidence intervals for effect size estimates. They exemplify such research as studies aiming to obtain accurate power estimates. Although their treatment of the issue is rather cursory, it is promising to see the scope of reproducibility research expanding to non-NHST approaches in broader science. However, it is pertinent that reproducibility research in ecology and CDM: we cannot evaluate the reproducibility of this body of work without first identifying relevant types of reproducibility issues.

------

**Chapter Two: QRPs for non-hypothesis testing research**

The first output for this research is to generate a "roadmap" of reproducibility issues, biases and questionable research practices (QRPs) that are encountered when developing Decision Support Systems in ecology and conservation.In this chapter I will attempt to identify where exactly in the DSS development process particular biases or 'decision-points' are likely to occur. This work should serve as a launching point for proposing standards and technical solutions targeted at individual (or teams of) researchers / analysts, with the broader objective of minimising the extent and magnitude of questionable research practices for DSS. 


This chapter will constitute (from what I am aware of) the first attempt to investigate reproducibility issues for non-hypothesis testing based research. To this extent, the knowledge resulting from this chapter should also be applicable to other fields of science beyond ecology and conservation where translational research and non-hypothesis testing methods are prevalent. It will also build on the work of Fraser et al. [-@Fraser:2018cl] in advancing research on reproducibility and transparency in ecology.

**INSERT ONE-TWO SENTENCES DESCRIBING QRPs / Hannah's paper**

*Methods for generating the roadmap*

1. Sketch out the SDM / DSS development process, but breakdown into modelling steps if necessary. As a starting point, I will take the Structured Decision Making process as the overarching framework for building a Decision Support System.
        a. Does this process differ for different tools / decision frameworks?
2. Identify sources of bias AND QRPs that others have identified in ecology and evolution, but also in other scientific disciplines and translational research fields
        a. at the individual DSS level
        b. at a higher level, e.g. in the evidence-base
        d. Are these biases / QRPs applicable to DSS's?
        e. are there other biases unique to DSS's that haven't been considered?
        f. Is their occurrence specific to the type of tool or application under consideration? i.e. are some tools more robust to biases / QRPs than others?
3. Map these biases / QRPs onto 1, where do they occur at the various decision-points?

I plan on utilising the outputs of the reproducibility discussion session planned for the Qaecera Retreat. The aim of the session is to initiate awareness and discussion among Qaecologists and Cebranalysts about reproducibility in our research practices, with the subsequent aim of equipping people with solutions to minimise or overcome these issues. The format for the session will involve structured / facilitated discussion in small break-out groups. Given the collaborative nature of this work, participants will be invited to co-author the paper after the retreat. This might also incentivise people to attend the session!

-------

# Evaluating the Transparency and likely reproducibility of decisions in ecology and conservation

Fidler et al. [-@Fidler:2017he] call for an assessment of the completeness and transparency of methodological and statistical reporting in journals for ecology. Such assessments should take the form of extensive journal surveys, with the aim of highlighting deficiencies in journal's reporting policies. Why is it important: Incomplete reporting impedes direct replication, meta-analysis, and also direct re-analysis projects. From personal experience, I have reason to believe that incomplete reporting is rife in environmental decision support systems. In working with Bayesian Belief Networks for catchment management, it was impossible to re-build all but one system model from the published literature. At best, the causal structure of the model is reproducible, but only one published study I encountered contained sufficient information so as to reproduce its parameterisation. Importantly, there is also the issue of understanding modeller choices about how and why variables were parameterised. This information is rarely recorded, and often the source of empirical data used in parameterisation is not reported. The next chapter of my research will take up this call, where I will conduct a systematic review aiming to systematically evaluate the decision support literature in ecology and conservation for its transparency and likely reproducibility.

------

**Chapter Three: Evaluating the transparency and reproducibility of Decision Support Systems in Ecology and Conservation**

The roadmap of QRPs developed in Chapter Two will inform both the scoping and evaluation criteria for the systematic review. The results of the review will be used to generate a 'reproducibility checklist' or set of criteria for DSS in ecology and conservation. The checklist will be aimed at individual researchers, but with the hope of being adopted by relevant journals. The checklist will need to balance a sufficiently acceptable set of standards against resource constraints on individual researchers in order to prevent the checklist from deterring important DSS work from being published.

**Methods:**

I will follow guidelines for undertaking a systematic review in the format and procedure established by Collaboration for Environmental Evidence (CEE) Evidence Synthesis [@CEE:M-Sg3G8U]. I chose this method of systematic review partially because the topic area is relevant (environmental management), and partially because of the comprehensiveness of the guidelines: the guidelines provides detailed and systematic methods for develpping the search strategy, inclusion / exclusion criteria, and coding criteria for evaluating the literature, including pilot searches for refining criteria. The other drawcard was the enforced development and preregistration of the protocol for conducting the review in order to prevent 'mission creep' as the review progresses.

In addition, I will use the PRISMA statement for reporting (CITE), as recommended by Nakagawa and Poulin [-@Nakagawa:2012fl]. The PRISMA checklist contains a checklist of 27 reporting items, as well as a flow diagram that visualizes the database searching procedure as well as decisions for including and evaluating studies. The aim of the statement is to increase the transparency of the literature review process.

**Review scope:**

Setting the scope - what is the unit of study? This is where the language thing is useful. This research will consider reproducibility of decisions generated from 'decision *frameworks*', not just from tools. It's important to consider the whole decision process, because CDM work indicates that problem formulation may influence the decision outcome (Law paper).

Ideas for what/how to analyse:

- Does 


SDM work only? No. Although SDM approaches are very common in conservation, I think that restricting the inclusion of studies to nominally SDM studies will omti a lot of research that might be decision-analytic in approach, and therefore still be be a structured / formal framework for decision-making. There's so much slippage around the classification terminology. Might be omitting relevant work. But also, I would like one aspect of the analysis to consider whether the framework used influences the likely reproducibiltiy / transparency of the decision.

There is such a huge diversity of tools, and I think you need to have good domain knowledge / expertise / familiarity with them to be able to give them proper treatment. Leave that work to the researchers using those particular tools. The PVA paper is a good example of one.
the *likely* reproducibility of studies.

------

**Chapter Four: case studies in replicating DSSs**

One thing to test for in the replication:

Bower et al [-@Bower:2017im] argue that problem formulation is a crucial first step in any CDM process. However, they state that " If parties to the decision process do not have a clear, shared idea of the problem itself, then entering into an SDM process is recommended. Specific techniques outlined in Table 1 can then be used to help clarify problem formulation." Page 4, [@Bower:2017im].  Wonder if that is robust enough, and how you measure the definition of the problem... line of inquiry for review.. Are decisions generated from DSS's developed with an SDM framework more reproducible than those that are not?

It's worth trying to get a handle on this... because these more rigorous decision protocols have a cost to them and "Unfortunately, insufficient resources often constrain smaller conservation agenciesâ€™ capacity for strictly following rigorous decision protocols." Page 4, [@Bower:2017im]. Bower et al suggest using truncated versions of these protocols, such that the decision process is still transparent.

**what's the point of this**

Fidler: re-analysis projects - both direct and conceptual re-analyses ("highlight the impact of disclosed and undisclosed statistical assumptions and versions or editions of software, [etc.]")

Conceptual re-analysis: "provide the opportunity to assess the extent of variability among analytic approaches and choices".

Comparative - compare a selection of different DSS problems.

**Method**

Two ideas

1. CJ - multiple river systems, opportunity for a red card type of experiment?

2. NESP -- Use my involvement in this project to examine QRPs sources of biases in a specific case study? What's the test?


------


# Ecological Informatics / Reproducibility Roadmap


3. Complex web of information flow. Through the DP itself from one step to the next, and new information going in at various points. Each step offering a chance for influence of biases in incoming data, transformed data, and cognitive biases of evidence and by the decision-maker and questionable research practices.

Look at reproducibility of decision support systems in ecology (not just decision-support tools). But contextualised within the broader web of the flow of information in the evidence-base.

Aims to generate tangible change at various levels

Michener et al define ecoinformatics as a "framework that enables scientists to generate new knowledge through innovative tools and approaches for discovering, managing, integrating, analyzing, visualising and preserving relevant biological, environmental, and socioeconomic data and information".

So perhaps I could extend an ecoinformatics model to account for translational research... i.e. one that accounts for the knowledge-doing gap.

PhD output: could be a thinkpiece, or might be the literature review or a chapter for the thesis (check the guidelines, you need one in addition to a published one)... But could be a real intervention... a technical solution.. like a small-scale / pilot, perhaps using Qaeco's task, data archiving repository that uses semantic annotation and ontologies.What would be the point of this?? There are already repositories in existance that do this... But do they do ontology-driven integration? Could build something that implements this... Payal's database problem.

What: Building an informatics for ecology (and decision suport systems as a part of this, to bridge the knowledge-doing gap).

Why: Sketching out this conceptual model of an informatics for ecology will aid me (and, actually, researchers / science in general) in highlighting potential avenues of future research. What parts of this 'macchine' can I focus on? What needs to be done.

Need to describe the  flow and transformation of information into knowledge, in a research 'ecosystem', But with sufficient detail at various levels e.g. enough insight into a single research pipeline for a single study, but also need to consider how data / information fits together at a disciplinary level / evidence-base level. E.g. We need data and resource sharing infrastructure, e.g. as suggested by [@Gribbin:2009us]: to prevent scientists from re-inventing the wheel [@Merali:2010tw], but importantly in a decision-tool context, to aid in evidence / data retrieval, selection to build models.

For conservation, there's this 'knowing-doing' gap. ANd an interesting set pf relations between practitioners and researcheres. Need to capture what Fiona called the  "translational research" element of decision-making reserach for conseration in our informatics.

Understanding the components of this system, and what their properties are... am I building an 'ontology'? See Culina paper and also Madin (2007)...: "Ontologies are formal models that define concepts and their relationships within a scientific domain such as ecology". Hmmm. Digging further.. not quite... More of a directed conceptual model.



### What makes ecological data special?
- [@White:2013ea]
- [@Borregaard:2018gu]
- [@Madin:2008jv] ontologies for ecology... talk about the specifics of ecological data. Concluding remarks section has good discussion on changing landscape of ecological work and data landscape.. integrative / synthetic approaches to dealing with data.
- [@Jones:2007fv] metadata-driven framework for generating field data entry interfaces in ecology
- "Page 1
One challenge that is particularly daunting lies in dealing with the scope of ecology and the enormous variability in scales that is encountered, spanning microbial community dynamics, communities of organisms inhabiting a single plant or square meter, and ecological Page 1
One challenge that is particularly daunting lies in dealing with the scope of ecology and the enormous variability in scales that is encountered, spanning microbial community dynamics, communities of organisms inhabiting a single plant or square meter, and ecological 
Page 2
processes occurring at the scale of the continent and biosphere. The diversity in scales studied and the ways in which studies are carried out results in large numbers of small, idiosyncratic data sets that accumulate from the thousands of scientists that collect relevant biological, ecological and environmental data
[18] 
Page 2
Such heterogeneity can be attributed, in part, to methodological specialization to address specific scientific hypotheses, but also to a lack of standard protocols for acquiring, organizing and describing data and language barriers and cultural differences across disciplines, institutions and countries.
"

How does ecology differ from other disciplines? Madin 2008 paper... lack of formalised terms and concepts... It's seen a a 'soft-science'




The data landscape of ecology is changing..
Newer synthetic approaches to ecological analyses, that are often cross-disciplinary (this was in 2008).

"Ecologists increasingly address questions at broader scales that have both scientific and societal relevance", [@Michener:2012ho]. Affected by changes occurring in science more broadly, and so there is growing emphasis on data stewardship sharing, openness [@Michener:2012ho].

Need to unpack what we do in decision support for ecology. what sorts of models / analysis do we use? not NHST. often building predictive models. Law paper. Conservation decision-making and evidence work

So, in conservation-decision making, we need to be aware of the technical issues around reusing and synthesising data -- Culina et al [-@Culina:2018dn] highlight an example where two different meta-analyses wrongly concluded tehre was no net loss of biodiversity due to spatial biases in collected data sets. 

### Problems (with regard to ecological data storage and accessibility)

Ecology's filedrawer problem:
In ecology and evolutionary biology, most data that is collected is lost to science, and are never accessible to anyone other than the original collector or user [@Whitlock:cl]. That excludes summaries posted in publications. Then, eventually many data are lost, even to the original collector [@Whitlock:cl].
But the foundation of science is data - information about the natural world obtained through experiment and observation.


Data collected, often for a targeted purpose, without considering how it might be reused for broader projects and analyses [@Madin:2008jv]. Aimed to be used within respective projects only. Data owners are the only intended users, and information about structure, content and usage is not recorded. IMO probably because the data creator has this mental understanding of the data and doesn't need it.
"As ecological research becomes holistic and integrative, better approaches are needed " [@Madin:2008jv]. In Conservation Decision Making, I think this was always already an issue. 

"Current data practices in ecology are not amenable to data sharing and re-use." Spreadsheet models, or even more sophisticated database frameworks don't have the necessary information to facilitate long-term preservation and interpretation of data. There are good approaches, but ontology-based approaches are rare. "Thus, the adoption of ontolo- gies is hindered both by the familiarity of current practices and the lack of tools to readily migrate to improved prac- tices."  [@Madin:2008jv]

"Long-tail" of ecological research [@Culina:2018dn]: "many individual projects producing small-scale data" has not embraced the open data movement. But some fields in ecology and evolution that are characterised by 'big data' have embraced open data. Authors think that it is the heterogeneous nature of ecological research (e.g. specific taxa, systems, regions or methodologies) that have impeded uptake.

**increasing use and establishment of data repositories / open data in Eco/Evo**
There is increasing demand for the use of open data in ecology and evolutionary biology. One example is that there is a need to "identify broader ecological and evolutionary patterns and processes across species, space and time " [@Culina:2018dn]. Two other such uses are 1. facilitating meta-analyses, and replications. 2. Conservation decision making and decision support!! Translational research of ecological research into decision support systems and then into decisions. 3. "re-analysis of data using new statistical iapproaches, error checking or use of existing data to address new questions "

Increasing data deposits are driven by policies of journals or funding agencies to encourage or require data archiving as part of the publication process [@Whitlock:cl]. Lots!!! of ecological repositories now (table 2 of [@Michener:2012ho])

The two functions of open data archiving [@Whitlock:cl]:
1. error checking and verification of results. As well as a method for preventing and correcting misconduct, although it is likely rare.
2. data to be re-used for broader meta-analyses, and for addressing new questions.

## Conservation evidence-base - "DATA LANDSCAPE"

Lots of good papers writing about this, I believe they can help inform my understanding of reproducibility issues for ecology



Conservation decision making according to Bower: "Conservation practitioners face complex challenges due to resource limitations, biological and socioeconomic trade-offs, involvement of diverse interest groups, and data deficiencies".

Bridging the knowledge-doing gap. This stuff will cover the evidence base / conservation literature.

Segan
Sutherland
Pulin
Bower


# Timeline of activities and goals

- timeline of activities and goals for first 12 months of candidature (GANTT chart),
including preparation of the confirmation report and holding the confirmation meeting (9 - 10 months)

# References
