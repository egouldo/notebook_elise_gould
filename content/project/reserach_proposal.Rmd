---
title: PhD Research Proposal
author: Elise Gould
date: '2017-02-15'
categories:
  - research proposal
tags:
  - project
output:
  html_document:
    keep_md: yes
bibliography: ../../static/files/citations/research_proposal.bib
slug: research_proposal
---

Max 5 pages, 3000 - 4000 words.
Send to committee and cc'd to rhd-biosciences@unimelb.edu.au

- are topic and aims well defined and achievable
- are the appropriate methods established or will they need to be developed
- does the student understand the relevant literature
- are there any intellectual property issues
- current / potential EHS issues?
- well-referenced intro that explains scientific context of project (1 - 3 pages)

Keywords:

- conservation
- decision-making
- ecology
- reproducibility
- structured decision-making

# Introduction

Successful biodiversity conservation and management is underpinned by effective and robust decision-making [@Mukherjee:2018cb]. Decision-makers are tasked with allocating limited resources in the face of uncertainty about the effectiveness of alternative management interventions, and incomplete or inadequate scientific information. Moreover, environmental decisions often must be made in complex socio-economic and political contexts, with multiple stakeholders and multiple and/or competing objectives. Formal, structured approaches to decision-making under uncertainty, such as Structured Decision Making, are commonly espoused as a means to more robust and reproducibile conservation decisions because of their transparency in the decision process (**LIST EXAMPLES**). However, this claim to assumed reproducibility remains untested, and a more nuanced consideration of what exactly reproducibility means in this application is lacking. Given that science is said to be in the grips of a "reproducibility crisis", and that biased and unfair scrutiny in the broader socio-political climate of "alternative facts" threatens credibility of scientific knowledge [@Baillieul:2018jd], it is both essential and timely that the reproducibility of decision-support in ecology and conservation is given rigorous research attention.

Science is a cumulative process building upon previous research to develop knowledge, however, this process requires that the findings are both real, and replicable [@Nakagawa:2015bn]. As the recent "reproducibility crisis" debate illustrates, this assumption is often incorrect [@Nature:un]. Failure to reproduce a large proportion of published studies in the fields of psychology and medicine has received considerable attention and provoked heated discussion among researchers in the broader scientific community. For example, the Open Science Collaboration's Psychology Reproducibility Project reported that less than half of published results could be reproduced [@OpenScienceCollaboration:2015cn]. 

Despite a growing body of large-scale meta-analyses across many different disciplines, debate as to whether there is a "crisis" persists. Fanelli et al. [-@Fanelli:2018je] use a failed replication of a large-scale meta-analysis to argue that the "crisis" is mistaken, and should instead be rebranded as a narratvive of "epochal changes and empowerment of scients" [@Jamieson:2018kz]. In a 'post-truth' era of 'alternative-facts', how scientists communicate research on the robustness of science and its self-correcting mechanisms is certainly important [@Sutherland:2017hc]. Although large-scale direct replications are largely absent in ecology [@Nakagawa:2015bn], an initial assessment of the conditions found to foster reproducibility problems provide evidence that ecology as a discipline is *at risk* of a "reproducibility crisis" [@Fidler:2017he]. **Moreover, CITE HANNAH'S PAPER HERE.** **CALL FOR METARESEARCH, fidler**.

## Reproducibility *of* decisions versus reproducibility *for* decision-making



Morrison et al. [-@Morrison:2016cd] propose some initial discussion as to the implications of the non-reproducibility of the technical inputs into the decision-making process.

- "reliability of predictions could result in scarce resources being directed inefficiently" + therefore opportunity costs
- credibility / repeatability of original model "those that are not repeatable may bring into question the validity and predictions of the original model."
- "where predictions cannot be repeated or reproduced practitioners may be unable to evaluate whether any conservation action or spending has achieved the desired conservation objective" Can't evaluate against predictions.
- Can't evaluate effectiveness of intervention against predictions of the mode (because you can't repeat the model!) AND this is really important in ecology because we only really ever get one chance n = 1 to test the effectiveness of an action, and then we must use the model outputs to to compare against the counter-factual.
- In an adaptive management context / management underpinned by monitoring, or during iterative model development if you can't reproduce the model, ability of conservation practitioners to compare revised models using updated parameters to previous models is limited.

These arguments mirror / echo those made in the conservation decision making literature and the role of the robustness of the evidence-base. Law, gardner, sutherland S4.

But what's missing? 

yes, further research like the Morrison work above, is needed on the reproducibility of inputs from models / tools at various stages of the decision-process. However, must examine the reproducibility of decisiosn through the *whole* decision process.

Reproducibility of the "raw" evidence-base is important. But why would it be worth looking at *decisions*? what are the implications?

But this is just focussing reproducibility issues *in* the evidence base... i.e. in terms of the reproducibility of the information / inputs going into the decision support system.

Why we need to examine reproducibility of decisions... but as embedded in a broader informatics-based framework.

Paper I sent to Libby about values in the DP: "Decision-making in conservation is a human enterprise. Inevitably, values and ver- ifiable facts shape conservation decision-making ".

Law: Risk attitudes + values, can impact the chosen decision. It's important that we examine the whole DP wrt reproducibility.

Then lead onto the major aims / scope of my PhD:

explore reproducibility of decisions generated from formal approaches to conservation decision-making. And will do so by examining knowledge / information synthesis within different parts of this knowledge system.

---------
new structure:

# Reviewing repro literature in eco cons.

## transparency and openness (primarily)

- individual solutions
- broader research context - openness standards - see parker and nakagawa TOP guidelines.

## Metaresearch (recently)

- more details about Fiona's work. QRP and broader culture.

## Expanding definitions of Repro
To better suti ecocons and translational research

### Non-NHST problems
### ROle and function of reproducibility for decisions

- defining reproducibility of decisions: warranted because no formal definition anywhere. Unpack the term. Touted as reproducible, unpack a little you'll see that it's 'repeatable'
- methodological reproducibility (with an emphasis on computational reproducibility and technical solutions) has been the focus in ecology thusfar, but there are other types.
- paragraph or two on typologies and functions of reproducibility. validity vs. generality.

# Project OUTLINE

------







-------

There is an inherent cost to ensure / increase reproducibility of an experiment or study [@Nature:un].. time (better documentation around methods for example) and therefore money. Very onerous to check code, eve.


# Reproducibility of Ecology and Conservation - State of the Research / small lit review
feasibility Parker and nakagawa.
What is the state of the 'crisis' in ecology? Extent...? How do we measure?
"broad term used to describe the ability to replicate an experiment or study and / or its outcomes." [@Fidler:2017he]
May be used inter-changably with the term 'replication'. 
Encompasses the ideas of repeatability (hard-copy book).
There are a number of different definitions / typologies of replications:
- [@Nakagawa:2015bn] 

See [@Williams:2017bg] for a good brief definition of replication / reproducibility using Peng.


### Meta-research

Initial reproducibility research in ecology has 

There is a growing body of work at the meta-research level, characterising the state of the problem at the disciplinary level.
In its infancy, work mostly focussed on trying to identify whether it's likely that we have a problem with reproducibility in ecology. Drawing on the body of work from fields in Medicine and Psychology, to a lesser extent Physics, Economics. Papers by Nakagawa, Parker, Fidler. These studies are synthesising across many ecological studies.

### Non-hypothesis testing - the unexplored depths of reproducibility research, and why it's so important in ecology and conservation

(** Note this is a logical follow on from meta-analyses in ecol because they have focused on NHST methods in EcoEvo only)

The reproducibility literature has focused exclusively on hypothesis-testing, whether that be Bayesian or frequentist. However, Fidler [-@Fidler:2016wv] correctly identifies that in applied ecological research, particularly in conservation science, non-hypothesis testing methods, such as decision-theory, cost-effectiveness analysis, optimization and other scientific computing methods are common. Moreover, these approaches come with their own set of reproducibility issues. However, a full understanding of the types of reproducibility issues as well as their impact on either the evidence-base, or the decisions informed by the evidence-base, is yet to emerge. 

In what is likely the first reproducibility study in conservation, Morrison et al. [-@Morrison:2016cd] evaluate the Population Viability Analysis (PVA) literature by performing direct tests of repeatability and reproducibility. They found that poor model parameter reporting practices meant a large number could not even be repeated, let alone reproduced. This work marks an important step towards identifying reproducibility issues relevant not only to PVA-based research, but also to CDM more broadly. Outside of ecology, where reproducibility research has received much more attention and focus, I could only identify a single paper that considered non-NHST based work in the reproducibility discussion. Crutzen and Peters [-@Crutzen:2017kn] suggest re-terming "power" and "underpowered" studies as being "undersamplesized", in a move to embrace the disciplinary shift away from NHST towards inferences based on confidence intervals for effect size estimates. They exemplify such research as studies aiming to obtain accurate power estimates. Although their treatment of the issue is rather cursory, it is promising to see the scope of reproducibility research expanding to non-NHST approaches in broader science. However, it is pertinent that reproducibility research in ecology and CDM: we cannot evaluate the reproducibility of this body of work without first identifying the relevant types of reproducibility issues.

**Task One**
Unpacking what this ("reproducibility issues")  means ... what do we need to know? what is my task
a. how to measure the likely reproducibility of a study
b. what is the function / role of different types of replications in terms of what they tell us about the broader state of the literature (validity, generalisations)
c. 

Falling under the banner of 'reproducibility' are a number of different concepts. Moreover, there is disagreement among the scientific community as to how to define what a reproducible study actually is.

### Computational Reproducibility

At the other end of the spectrum there is a decent body of work focusing on the level of the individual researcher / study. This literature examines the data analysis pipeline for a single analysis or study, and is largely software and data focussed. It could be described as falling under the banner of "reproducible research", "scientific computing". It's technically-minded and solutions-focussed, and seems to really be drawing on the notion of reproducibility as being computational or direct in nature, rather than on replications. Solutions are largely software-based. And take the starting point of reproducibility to be at the point where analysis is ready to begin (you've got the data already!). Draws largely on the work of computational biology. Moreover, seems to be relevant to the work of individual researchers and smaller research groups, with much discussion in the grey literature, particularly in blog format.

**Examples:**
- [@Wilson:2014ck] describes a set of practices for improving reproducibility of analyses in many quantitative research settings, e.g. version control, incremental changes, functional programming, plannning for mistakes using unit-testing...etc.
- literatre programming and Rmarkdown documents for portable, containerised analyses linked to their inferences, e.g.: [@Baumer:2015hc] and [@Gandrud:2016ux]
- [@White:2013ea] how to make your data re-usable to others for replications, or whatever. But also on how to maek it reproducible for yourself. emphasis on ecological data (types.)
- BES's guide to data management [@BritishEcologicalSociety:2014va]
- Organizing computaitonal biology projects [@Noble:2009da] not ecol specific, but biol specific
- Best practices for reporting climate data in ecology. Guideliens for individual researchers, but witha  view of creating transparency and reporting standards for editors, and reviewers [@MoruetaHolme:2018bi]
- [@Evolution:cl] describe technical and cultural issues that need to be addressed to aid archiving more data in Eco/Evo. target the best-practice guideliens for data creators, data users (i.e. researchers synthesising data), and for journal editors and publishers.
- again another ecoinformatics paper, breaking down the research data life-cycle, 

Then there is research focussing on the the reproducibility of particular tools or methods used to inform conservation decision-making, e.g. for Population Viability Analyses PVA. Morrison et al 2018 frontiers in ecology and evolution. [@Morrison:2016cd]. This is a step towards understanding that reproducible or "robust" decisions aren't just those that follow a transparent process... but "scientific input into conservation plannign [must be] robust and reliable, thereby increasing the chances of making decisions that are both beneficial and defensible". So it views decisions in this broader context. Also! reallly cool paper because it tried to directly repeat and reproduce a bunch of PVA's systematically. cool.


### Transparency, open data and sharing

Then there's also a push to *share* data in ecology "open data" and science more broaldy. this work is less likely to only consider the researcher / study in its own right, and consider issues of reproducibility within a broader systems-level view: treating openness as precurser for a) fostering greater rates of replication, by making individual studies more readily reproducible (computationally / directly) [@Merali:2010tw]. Importantly, there is recognition that issues around reproducibility and openness as pertaining to the technical limitations of individual researchers are actually inter-generational and cultural: As ecologists, scientists more generally, we mostly aren't trained in software development, but have picked bits and pieces up along the way / are self-taught (i.e. we lack a formal training) [@Merali:2010tw]. The same point has been made about statistical training of ecologsts (CITE... context of QRPs???). Consequently proponents of openness also advocate for collaboration of ecologists with software developers and computer scientists and for bringing industrial software practices into the workflows of labs and research groups [@Merali:2010tw]. 

QRP study: [@Agnoli:2017kl]

Some proponents of openness argue that transparency (access to source code, and links to data) is not enough -- too onerous to have researchers check data line by line in replications [@Merali:2010tw]. There are technical solutions from the Computer science / reproducible research areas that aim to alleviate this barrier to checking the reproducibility of papers, e.g. E.g. $R^2$ the executable papers platform for the R community [@Leisch:2011hf]. Aimed to remove the burden of checking computational reproducibility off reviewers by providing a "new web service which outsources validation of computational results in executable papers to an independent third party." It's one solution to the issue of reviewers and journals having to check this (some journals have hired statiscial reviewrs and programmers to address this sticking point for reproducibility (CITE)).


"The combination of for- mal systems for tracking provenance, and fed- erated data repositories like DataONE that provide unique identifiers for every data object, will be instrumental in realizing the goal of fully reproduc- ible science in support of understanding global environmental issues such as climate change, species invasions, and epidemics" data sharing should facilitate "as well as greater reproducibility and transparency of the methods and analyses that support scientific insights."


Openness work still largely considers reproducibility to be computational or direct reproducibility.

likely to consider transparency and openness as precursers for replications. Not only considering computational reproducibility.

Openness work Still technically focussed:
- individual reseracher solutions to facilitating data-sharing  in ECOLOGY (e.g. meta-data), but with a view to combining data with other data in the future - i.e. facilitating longitudinal studies, reuse for other questions (other data parasites), or replications  / reproducibility studies [@White:2013ea] 
- [@TorontoInternationalDataReleaseWorkshopAuthors:2009jh] Pre-publication data-sharing. 
- [@Anonymous:a1kBf1q9] "Git can facilitate greater reproducibility and increased transparency in science"
-  Not enough just to provide source code and links to data. Get scientists and software developers working together. Develop code in modular fashion so that can easily add new analyses. Bring Computer scientists into research groups.
- [@Merali:2010tw] Lots of technical solutions. Short survey of the opennes problem and how it impedes reproducibility. Examines issues around transparency with a focus on scientific software and code development by scientists.
- Calls for scientists to publish computer code with paper [@Barnes:2010dn], with solutions focus on the individual researcher.

Other Openness papers Policy-focussed approaches considering reproducibility and studies in the context of a broader ecosystem of research in which reproducibility is impacted by some broader culture:
- Other calls looking at barriers / lack of standards beyond the individual researcher that prevent scientists publishing code - AKA "source-code withholding" (institutional support, funding policy, publishing requirement - journal level) [@Science:2012hf].
- Scientific computing studies also consider this broader ecosystem (R2 paper), and address issues of policy: [@Stodden:2012ww]. Highlights jouranl poolicy and funding agency, but misses publishing requirements, as the paper above included. This study also argues that it's not just about source-code but about the whole process, from the dev environment to the complete set of instructions that generated the figures, and paper.
- post-publication data sharing: publication level standards / policy, licensing schema and accompanying data and resource-sharing infrastructure, incentives [@Gribbin:2009us]
- Empirical evidence looking at trends in openness policy in terms of data sharing: [@Stodden:2013ho].



Both metaresearch work and reproducible research / open science / data advocates tend to consider reserach in the broader context, and offer policy-based approaches that target the journal editors, funding agencies, and publishing requirements (some technical, e.g. post-publication paper).


# Task 1 - What is "reproducibility" for Decision Support Systems? Defining reproducibility for Decision Support Systems in Ecology and Conservation
### Where are we up to? Unpacking "reproducibility" for decisions Reproducibility for CDM is taken to mean 'repeatibility'

Transparency is frequently touted as both a key-feature of environmental decision-support systems, and as central to delivering reproducible decisions [@Kim:2016gf] (OTHER REFS, basically anything SDM). However, the discussion of reproducibility in these contexts usually stops here, and there is no further discussion as to some definition of the term. [-@McIntosh:2011ew] elaborates further than most to explain the link between transparency and reproducibility in formal decision support for ecology: 

> Transparent because rational explanations can be provided to support decisions, and because the user/stakeholder/citizen can reproduce the decision procedure, play with the weights, and perform sensitivity analysis to assess decision strength and robustness.

Examining this statement reveals that, at least for these authors, reproducibility is simply the repeatability of the decision procedure. Repeatability *is* important, and the assumption that the transparency of decision processes implemented informal decision support systems results in them being repeatable should be tested -- especially given how commonly it is used as a selling-point for particular approaches, like SDM. However, I believe that a more thorough treatment should be given to defining reproducibility for decision support in ecology and conservation.

Firstly, the repeatability of a decision procedure doesn't necessarily ensure that the resultant decision 

Needs to be a definition of what the outcome of interest is, in terms of reproducibility - procedure, or the decision?

1. Just because you can repeat a procedure, doesn't mean that the decision deriving from the procedure is going to be reproduced. Expect that the more complicated the procedure, (more tools, more sophisticated tools), and the more complex the decision problem and domain the

Repeatability is important. But who says that just because you can repeat a procedure, that the decision deriving from that procedure is going to be the same.
falling under a direct replication.
Moreover, this is just one definition of reproducibility. Many different typologies in existance. Computational or "direct" replications.

### What's next?

Can't begin to evaluate the reproducibility of decisions without some definition, and measure of what that might be. Since the outputs of EDS are not p-values or whatnot, we need to decide what they might be. 

Furthermore, we need to decide on what the role or function of reproducibility of different types of reproducibility might be.

Increasing discussion of QRPs in ecology. But how might they apply to 



Significant resources are invested in conservation and management. 
1. demonstrating credibility is important.
2. 

What is the risk that decision support tools are irreproducible? I.e. the consequences. Why is it worth examining.
A quick set of definitions / glossary (table): define them here in a table.
Now move onto decision support tools - huge part of what we do in applied ecology. 
So how might a reproducibility crisis manifest in ecological decision support tools?
Decision models are built against an uncertain evidence base [@Law:2017ia].

# Project Aims and Outline 

- an outline of the major aims of the project and the approaches that will be taken (1 - 2 pages)

Provide further insight and deepen the understanding of issues of reproducibility and transparency in ecology and conservation decision making.

1. Develop an understanding / framework / definition of reproducibility that is applicable to Non-NHST and translational research, that is sensitive to the particularities of the data, the methods, the questions commonly used within these contexts.
2. Build a picture / conceptual model of ecological informatics with a reproducibility bent, multi-level.
2. Characterise the extent and magnitude of the "problem."reproducibility crisis"
3. Devise (and possibly test) interventions for improving reproducibility and transparency in ecology and conservation. improving the scientific process in ECO / CDM. Could be policy-based, could be guidelines  targeted at the author, could be a technical tool.


scope: Am i examining just models... or am I looking at the entire decision-making process... from specification of objectives, for example.. My feeling is that there are a lot of papers focusing on good / robust modelling practices within ecology / conservation in general, as well as articulating why models are necessary in decision-making. I think it could be useful to consider the entire process to developing the decision tool as a whole because aspects of the whole process may influence the final decision.. For example, Law et al.  [-@Law:2017ia] argue that decisions are sensitive to the scenarios selected during scenario analysis.

## Paper 1 - QRPs for non-NHST-based Research (in EcoCons)

Contribution to broader research: Will be the first set of work attempting to understand reproducibility issues for non-hypothesis testing based research, and also will build on the work of Fraser et al. to understand reproducibility issues in ecology (and conservation, not yet done).

Contribution to PhD: This paper will build a picture or 'map' of QRPs in the decision process. The work will inform the scoping and evaluation criteria for the systematic review, whereby it will be used to generate a 'reproducibility checklist' or set of criteria that measure the transparency and the *likely* reproducibility of studies.

## Paper 2 - Systematic Review

The aim of the review is to evaluate the decision making literature in ecology and conservation for its transparency and potential reproducibility.

Setting the scope - what is the unit of study? This is where the language thing is useful. This research will consider reproducibility of decisions generated from 'decision *frameworks*', not just from tools. It's important to consider the whole decision process, because CDM work indicates that problem formulation may influence the decision outcome (Law paper).

Moreover: Researchers, particularly those trained in the natural sciences


SDM work only? No. Although SDM approaches are very common in conservation, I think that restricting the inclusion of studies to nominally SDM studies will omti a lot of research that might be decision-analytic in approach, and therefore still be be a structured / formal framework for decision-making. There's so much slippage around the classification terminology. Might be omitting relevant work. But also, I would like one aspect of the analysis to consider whether the framework used influences the likely reproducibiltiy / transparency of the decision.

There is such a huge diversity of tools, and I think you need to have good domain knowledge / expertise / familiarity with them to be able to give them proper treatment. Leave that work to the researchers using those particular tools. The PVA paper is a good example of one.

## Paper 3 - DSS Protocol Development

## Paper 4 - How do you replicate a DST?

One thing to test for in the replication:

Bower et al [-@Bower:2017im] argue that problem formulation is a crucial first step in any CDM process. However, they state that " If parties to the decision process do not have a clear, shared idea of the problem itself, then entering into an SDM process is recommended. Specific techniques outlined in Table 1 can then be used to help clarify problem formulation." Page 4, [@Bower:2017im].  Wonder if that is robust enough, and how you measure the definition of the problem... line of inquiry for review.. Are decisions generated from DSS's developed with an SDM framework more reproducible than those that are not?

It's worth trying to get a handle on this... because these more rigorous decision protocols have a cost to them and "Unfortunately, insufficient resources often constrain smaller conservation agencies’ capacity for strictly following rigorous decision protocols." Page 4, [@Bower:2017im]. Bower et al suggest using truncated versions of these protocols, such that the decision process is still transparent.

# Ecological Informatics / Reproducibility Roadmap

Michener et al define ecoinformatics as a "framework that enables scientists to generate new knowledge through innovative tools and approaches for discovering, managing, integrating, analyzing, visualising and preserving relevant biological, environmental, and socioeconomic data and information".

So perhaps I could extend an ecoinformatics model to account for translational research... i.e. one that accounts for the knowledge-doing gap.

PhD output: could be a thinkpiece, or might be the literature review or a chapter for the thesis (check the guidelines, you need one in addition to a published one)... But could be a real intervention... a technical solution.. like a small-scale / pilot, perhaps using Qaeco's task, data archiving repository that uses semantic annotation and ontologies.What would be the point of this?? There are already repositories in existance that do this... But do they do ontology-driven integration? Could build something that implements this... Payal's database problem.

What: Building an informatics for ecology (and decision suport systems as a part of this, to bridge the knowledge-doing gap).

Why: Sketching out this conceptual model of an informatics for ecology will aid me (and, actually, researchers / science in general) in highlighting potential avenues of future research. What parts of this 'macchine' can I focus on? What needs to be done.

Need to describe the  flow and transformation of information into knowledge, in a research 'ecosystem', But with sufficient detail at various levels e.g. enough insight into a single research pipeline for a single study, but also need to consider how data / information fits together at a disciplinary level / evidence-base level. E.g. We need data and resource sharing infrastructure, e.g. as suggested by [@Gribbin:2009us]: to prevent scientists from re-inventing the wheel [@Merali:2010tw], but importantly in a decision-tool context, to aid in evidence / data retrieval, selection to build models.

For conservation, there's this 'knowing-doing' gap. ANd an interesting set pf relations between practitioners and researcheres. Need to capture what Fiona called the  "translational research" element of decision-making reserach for conseration in our informatics.

Understanding the components of this system, and what their properties are... am I building an 'ontology'? See Culina paper and also Madin (2007)...: "Ontologies are formal models that define concepts and their relationships within a scientific domain such as ecology". Hmmm. Digging further.. not quite... More of a directed conceptual model.

## What makes ecological data special?
- [@White:2013ea]
- [@Borregaard:2018gu]
- [@Madin:2008jv] ontologies for ecology... talk about the specifics of ecological data. Concluding remarks section has good discussion on changing landscape of ecological work and data landscape.. integrative / synthetic approaches to dealing with data.
- [@Jones:2007fv] metadata-driven framework for generating field data entry interfaces in ecology
- "Page 1
One challenge that is particularly daunting lies in dealing with the scope of ecology and the enormous variability in scales that is encountered, spanning microbial community dynamics, communities of organisms inhabiting a single plant or square meter, and ecological Page 1
One challenge that is particularly daunting lies in dealing with the scope of ecology and the enormous variability in scales that is encountered, spanning microbial community dynamics, communities of organisms inhabiting a single plant or square meter, and ecological 
Page 2
processes occurring at the scale of the continent and biosphere. The diversity in scales studied and the ways in which studies are carried out results in large numbers of small, idiosyncratic data sets that accumulate from the thousands of scientists that collect relevant biological, ecological and environmental data
[18] 
Page 2
Such heterogeneity can be attributed, in part, to methodological specialization to address specific scientific hypotheses, but also to a lack of standard protocols for acquiring, organizing and describing data and language barriers and cultural differences across disciplines, institutions and countries.
"

How does ecology differ from other disciplines? Madin 2008 paper... lack of formalised terms and concepts... It's seen a a 'soft-science'




The data landscape of ecology is changing..
Newer synthetic approaches to ecological analyses, that are often cross-disciplinary (this was in 2008).

"Ecologists increasingly address questions at broader scales that have both scientific and societal relevance", [@Michener:2012ho]. Affected by changes occurring in science more broadly, and so there is growing emphasis on data stewardship sharing, openness [@Michener:2012ho].


## Problems (with regard to ecological data storage and accessibility)

Ecology's filedrawer problem:
In ecology and evolutionary biology, most data that is collected is lost to science, and are never accessible to anyone other than the original collector or user [@Evolution:cl]. That excludes summaries posted in publications. Then, eventually many data are lost, even to the original collector [@Evolution:cl].
But the foundation of science is data - information about the natural world obtained through experiment and observation.


Data collected, often for a targeted purpose, without considering how it might be reused for broader projects and analyses [@Madin:2008jv]. Aimed to be used within respective projects only. Data owners are the only intended users, and information about structure, content and usage is not recorded. IMO probably because the data creator has this mental understanding of the data and doesn't need it.
"As ecological research becomes holistic and integrative, better approaches are needed " [@Madin:2008jv]. In Conservation Decision Making, I think this was always already an issue. 

"Current data practices in ecology are not amenable to data sharing and re-use." Spreadsheet models, or even more sophisticated database frameworks don't have the necessary information to facilitate long-term preservation and interpretation of data. There are good approaches, but ontology-based approaches are rare. "Thus, the adoption of ontolo- gies is hindered both by the familiarity of current practices and the lack of tools to readily migrate to improved prac- tices."  [@Madin:2008jv]

"Long-tail" of ecological research [@Culina:2018dn]: "many individual projects producing small-scale data" has not embraced the open data movement. But some fields in ecology and evolution that are characterised by 'big data' have embraced open data. Authors think that it is the heterogeneous nature of ecological research (e.g. specific taxa, systems, regions or methodologies) that have impeded uptake.

**increasing use and establishment of data repositories / open data in Eco/Evo**
There is increasing demand for the use of open data in ecology and evolutionary biology. One example is that there is a need to "identify broader ecological and evolutionary patterns and processes across species, space and time " [@Culina:2018dn]. Two other such uses are 1. facilitating meta-analyses, and replications. 2. Conservation decision making and decision support!! Translational research of ecological research into decision support systems and then into decisions. 3. "re-analysis of data using new statistical iapproaches, error checking or use of existing data to address new questions "

Increasing data deposits are driven by policies of journals or funding agencies to encourage or require data archiving as part of the publication process [@Evolution:cl]. Lots!!! of ecological repositories now (table 2 of [@Michener:2012ho])

The two functions of open data archiving [@Evolution:cl]:
1. error checking and verification of results. As well as a method for preventing and correcting misconduct, although it is likely rare.
2. data to be re-used for broader meta-analyses, and for addressing new questions.

## What makes applied ecological work / conservation special?

Need to unpack what we do in decision support for ecology. what sorts of models / analysis do we use? not NHST. often building predictive models. Law paper. Conservation decision-making and evidence work

So, in conservation-decision making, we need to be aware of the technical issues around reusing and synthesising data -- Culina et al [-@Culina:2018dn] highlight an example where two different meta-analyses wrongly concluded tehre was no net loss of biodiversity due to spatial biases in collected data sets. 

## Conservation evidence-base - "DATA LANDSCAPE"
Lots of good papers writing about this, I believe they can help inform my understanding of reproducibility issues for ecology



### Conservation Decision-making and translational research

Conservation decision making according to Bower: "Conservation practitioners face complex challenges due to resource limitations, biological and socioeconomic trade-offs, involvement of diverse interest groups, and data deficiencies".

Bridging the knowledge-doing gap. This stuff will cover the evidence base / conservation literature.

Segan
Sutherland
Pulin
Bower



# Timeline of activities and goals

- timeline of activities and goals for first 12 months of candidature (GANTT chart),
including preparation of the confirmation report and holding the confirmation meeting (9 - 10 months)

# References
