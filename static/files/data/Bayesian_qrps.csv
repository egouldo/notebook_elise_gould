Method,QRP,Source,Applicable to Frequentist,Reason is QRP,Other commentBayesian estimation,"P-hacking:  inc data-peeking, dropping conditions or covariates,  dropping outliers",[@Simonsohn:2014ka],Y,inflates risk of type I error to the same magnitude as frequentist methods,Bayesian P values (e.g. used in model checking),Could theoretically be prone to P-hacking,See discussion in [@Conn:2018hd],Y,As above,"Conn et al demonstrated that posterior predictive P values used in model checking (as well as other metrices) ""can have a larger than nominal \alpha value, so that our ability to ""reject"" the null hypothesis that data arose from the model is over-stated."" In asecond case study (spatial regression), ""the Bayesian P value often failed to reject models without spatial structure even when data were simulated with considerable spatial autocorrelation. The overstated probability of rejection is due to the double use of data, which are used both to fit the model and also to calculate a tail probability. """Bayes factors,As above,As above,Y,As above,Bayesian Hierarchical Modelling,Failure to undertake and / or report model checking,[@Conn:2018hd],Y,"Not enough to report MCMC chain convergence to a stationary distribution only: ""Perhaps there is a mistaken belief among authors and reviewers that convergence to a stationary dis- tribution, combined with a lack of prior sensitivity, implies that a model fits the data. In reality, convergence diagnostics such as trace plots only allow us to check the algorithm for fitting the model, not the model itself"" [@Conn:2018hd].","""Results of such goodness-of-fit tests are routinely reported when publishing analyses in the ecological literature. The implicit requirement that one conduct model checking exercises is not often adhered to when reporting results of Bayesian analyses. For instance, a search of Ecology articles published in 2014 indicated that only 25% of articles employing Bayesian analysis on real data sets reported any model checking or goodness-of-fit testing""  [@Conn:2018hd]"Bayesian Hierarchical Modelling,Selective reporting of model checking,[@Conn:2018hd],Y,,"""As in the case of “P hacking” (Head et al. 2015), care should be taken to choose appropriate goodness of fit measures without first peeking at results (i.e., employing multiple discrepancy measures and only reporting those that indicate adequate fit)."" [@Conn:2018hd]. Their paper demonstrates that different model checking methods may be better at diagnosing the particular causes or regions in the model where there is a lack of fit. THe plethora of choices for model checking givse rise to researcher degrees of freedom, and hence the very real potential for P-hacking and its equivalents."Bayesian Hierarchical Modelling,Failure to report convergence diagnostic of MCMC chains,[@Conn:2018hd],N,,"""all of the articles we examined did a commendable job in reporting convergence diagnostics to support their contention that MCMC chains had reached their stationary distribution"" [@Conn:2018hd]."Bayesian Hierarchical Modelling,Selective reporting of convergence diagnostics,[@Conn:2018hd],N,,